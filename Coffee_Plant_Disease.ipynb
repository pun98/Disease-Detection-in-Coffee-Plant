{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#library used to perform transfer learning\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\"\"\"Creates the data generators to be used in training the network\n",
    "Args\n",
    "    dataset: location of the dataset\n",
    "    image_dim: scale to this image dimension\n",
    "    batch_size: batch size\n",
    "Returns\n",
    "    train_generator: ImageDataGenerator\n",
    "tf.keras.preprocessing : slow in training\n",
    "\"\"\"\n",
    "def load_generators(dataset_dir: str, image_dim: Tuple[int, int], batch_size: int):\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        dataset_dir,\n",
    "        shuffle=True,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=image_dim,\n",
    "        batch_size=batch_size)\n",
    "    # it takes input images and outputs augmented images\n",
    "\n",
    "    print('created dataset for {}'.format(dataset_dir))\n",
    "\n",
    "    return train_generator\n",
    "\n",
    "#This function performs the data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1539 images belonging to 5 classes.\n",
      "created dataset for /kaggle/input/coffee-dataset/coffee-datasets/symptom/train\n",
      "Found 335 images belonging to 5 classes.\n",
      "created dataset for /kaggle/input/coffee-dataset/coffee-datasets/symptom/val\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "#This is the default input size for Inceptionv3 architecture\n",
    "\n",
    "# set the train and validation datasets\n",
    "train_dir = '/kaggle/input/coffee-dataset/coffee-datasets/symptom/train'\n",
    "validation_dir = '/kaggle/input/coffee-dataset/coffee-datasets/symptom/val'\n",
    "\n",
    "# load the data generators\n",
    "train_datagen = load_generators(train_dir, (IMG_HEIGHT, IMG_WIDTH), BATCH_SIZE)\n",
    "\n",
    "validation_datagen = load_generators(validation_dir, (IMG_HEIGHT, IMG_WIDTH), BATCH_SIZE)\n",
    "#These new datasets are new transformed images produced using ImageDataGenerator class required for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCEPTIONV3_TFHUB = 'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4'\n",
    "#Getting the InceptionV3 architecture model that is 47 layer deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "num_classes = train_datagen.num_classes\n",
    "#declaring the classes and input shape\n",
    "\n",
    "# fetch the feature extractor from the tf_hub\n",
    "feature_extractor = hub.KerasLayer(INCEPTIONV3_TFHUB, input_shape=input_shape)\n",
    "#used to gather the transfer learning model\n",
    "\n",
    "# make the feature extractor trainable\n",
    "feature_extractor.trainable = True\n",
    "# create the sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor, \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
    "])\n",
    "#creating a sequential \n",
    "#contains the inceptionv3 model along with a flatten and an Dense layer added to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 21,813,029\n",
      "Trainable params: 21,778,597\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
    "    #stochastic gradient descent optimizer\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 64 steps, validate for 14 steps\n",
      "Epoch 1/20\n",
      "64/64 [==============================] - 26s 400ms/step - loss: 0.7483 - accuracy: 0.8205 - val_loss: 1.8821 - val_accuracy: 0.5493\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 15s 229ms/step - loss: 0.4409 - accuracy: 0.9360 - val_loss: 1.8344 - val_accuracy: 0.6567\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 15s 227ms/step - loss: 0.3142 - accuracy: 0.9782 - val_loss: 0.4931 - val_accuracy: 0.9194\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 15s 228ms/step - loss: 0.2792 - accuracy: 0.9901 - val_loss: 0.4848 - val_accuracy: 0.9284\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 15s 227ms/step - loss: 0.2562 - accuracy: 0.9947 - val_loss: 0.3788 - val_accuracy: 0.9672\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 15s 229ms/step - loss: 0.2391 - accuracy: 0.9987 - val_loss: 0.3507 - val_accuracy: 0.9701\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 14s 226ms/step - loss: 0.2359 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9761\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 14s 226ms/step - loss: 0.2351 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9791\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 15s 228ms/step - loss: 0.2348 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.9791\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 15s 229ms/step - loss: 0.2346 - accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9791\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 14s 226ms/step - loss: 0.2345 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 0.9761\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 15s 227ms/step - loss: 0.2343 - accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9761\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 15s 228ms/step - loss: 0.2342 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 0.9761\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 15s 230ms/step - loss: 0.2341 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9761\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 15s 229ms/step - loss: 0.2340 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9761\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 15s 227ms/step - loss: 0.2339 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.9761\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 14s 226ms/step - loss: 0.2338 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.9761\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 15s 231ms/step - loss: 0.2337 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.9761\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 15s 227ms/step - loss: 0.2336 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9761\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 15s 230ms/step - loss: 0.2334 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4ba9082320>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(\n",
    "    train_datagen,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=train_datagen.samples//train_datagen.batch_size,\n",
    "    validation_data=validation_datagen,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 88ms/step - loss: 0.3414 - accuracy: 0.9761\n",
      "65/65 [==============================] - 6s 87ms/step - loss: 0.2339 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(validation_datagen)\n",
    "# train accuracy\n",
    "train_loss, train_accuracy = model.evaluate(train_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trained Model for 2 epochs, train accuracy: 100.00%, test accuracy: 97.61%'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Trained Model for {} epochs, train accuracy: {:5.2f}%, test accuracy: {:5.2f}%\".format(2, 100*train_accuracy, 100*accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
